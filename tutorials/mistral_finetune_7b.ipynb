{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyuOCYM92LJb"
      },
      "source": [
        "# Getting Started with Fine-Tuning Mistral 7B\n",
        "\n",
        "This notebook shows you a simple example of how to LoRA finetune Mistral 7B. You can run this notebook in Google Colab with Pro + account with A100 and 40GB RAM.\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github.com/24p11/recode-with-mistral-finetune/blob/main/tutorials/mistral_finetune_7b.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "\n",
        "Check out `mistral-finetune` Github repo to learn more: https://github.com/mistralai/mistral-finetune/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxr8mv-17GfB"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Clone the `mistral-finetune` repo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIj3IlIeVDIb",
        "outputId": "14f48f90-2915-4939-d053-5247a90a31f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'recode-with-mistral-finetune'...\n",
            "remote: Enumerating objects: 500, done.\u001b[K\n",
            "remote: Counting objects: 100% (500/500), done.\u001b[K\n",
            "remote: Compressing objects: 100% (226/226), done.\u001b[K\n",
            "remote: Total 500 (delta 266), reused 493 (delta 259), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (500/500), 1016.34 KiB | 7.31 MiB/s, done.\n",
            "Resolving deltas: 100% (266/266), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/24p11/recode-with-mistral-finetune.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQPd_pGT7WiY"
      },
      "source": [
        "Install all required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KuTOGipl7BS7",
        "outputId": "6e7e13b3-68ef-4f9e-aa2f-bbc466d09d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire (from -r /content/recode-with-mistral-finetune/requirements.txt (line 1))\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from -r /content/recode-with-mistral-finetune/requirements.txt (line 2)) (0.1.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r /content/recode-with-mistral-finetune/requirements.txt (line 3)) (6.0.2)\n",
            "Collecting mistral-common>=1.3.1 (from -r /content/recode-with-mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading mistral_common-1.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from -r /content/recode-with-mistral-finetune/requirements.txt (line 5)) (0.4.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (2.17.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r /content/recode-with-mistral-finetune/requirements.txt (line 7)) (4.67.1)\n",
            "Collecting torch==2.2 (from -r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting triton==2.2 (from -r /content/recode-with-mistral-finetune/requirements.txt (line 10))\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting xformers==0.0.24 (from -r /content/recode-with-mistral-finetune/requirements.txt (line 11))\n",
            "  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9)) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.24->-r /content/recode-with-mistral-finetune/requirements.txt (line 11)) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9)) (12.6.85)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r /content/recode-with-mistral-finetune/requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->-r /content/recode-with-mistral-finetune/requirements.txt (line 2)) (0.16)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (4.23.0)\n",
            "Collecting pillow<11.0.0,>=10.3.0 (from mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (2.10.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (0.2.0)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4))\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (0.22.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.1->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.1->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (2024.12.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->mistral-common>=1.3.1->-r /content/recode-with-mistral-finetune/requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r /content/recode-with-mistral-finetune/requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2->-r /content/recode-with-mistral-finetune/requirements.txt (line 9)) (1.3.0)\n",
            "Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.2/218.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.5.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=d17573d9eb79915ce828c62bbb1fe1215ad4695b17e4fa134cd10f87127ec60c\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built fire\n",
            "Installing collected packages: triton, pillow, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, tiktoken, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, xformers, mistral-common\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
            "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fire-0.7.0 mistral-common-1.5.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 pillow-10.4.0 tiktoken-0.7.0 torch-2.2.0 triton-2.2.0 xformers-0.0.24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "6e0bb8f613884f56a14b907f11af8c6c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r /content/recode-with-mistral-finetune/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgdIAi257jLo"
      },
      "source": [
        "## Model download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EBAiYGrvCJQt",
        "outputId": "41520980-ffae-47b7-f9ec-06e6f722e28f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UhWCRzfYCJQt",
        "outputId": "63d02da0-419a-4a3b-cb1e-c95dfc106ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "e9bb325daa4e468ca6692a5fa56fa746",
            "756aefba131c4f96b38b91189e75388c",
            "2bbbcfba8d4b44faa10513dc0c4fedb9",
            "c9c91aaf3c8a49cea2cfed5ebd380dcf",
            "796dcd2a671349c2bc52a138f34a7f00",
            "b53c43d389c24541b9529bfe916471d7",
            "7a8a1720cd724e87ae7adc6e022367b7",
            "330828c363554b3793c8dd6b552ec1e4",
            "9a95b97f66ea452c9618a79538cb5c27",
            "a5f08c3c23904be7965d3393ab69b5eb",
            "d76c4a231cee4a5f8a64fdea87fcbb29",
            "5867f4d42ea547fab863ea19186d3e2a",
            "ed61d658b5d549e4bb42abc1e12e6c39",
            "4d38049c50ad485c8d554c20bfbf5db4",
            "583231823e90487690e4a32d119db6f9",
            "6928ca3cbd3b4ff49ed42fa648f8e57c",
            "0b068494b0094ee3a3c9ea601ad7b145",
            "38a5c2c081f14330a7eab7535113523f",
            "506cd5ade01945b4af5f6f21d27fe3c6",
            "4929556aee13456bb02efeec925f24d3"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9bb325daa4e468ca6692a5fa56fa746"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# huggingface login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "8a550218d4314318a7d96442049568c3",
            "90a3e15f8bd742a681649132e7792304",
            "474d9603ae624e40a6ff3bcf6d1c98de",
            "3c2d923788754ed4bd27f45cf8d1d62b",
            "8d122899fb6a4b8a8cca4f1276b89a6f",
            "4faa0da877664304a52954e037f9f3a6",
            "9ff61767564148fdbc14ebf2b60b4a96",
            "d70907b99fec4ba98cdc5af471bf7156",
            "6724778febe74ce6b582e40b077e2806",
            "f637a4487d6040d2b87f6e796a38b5ce",
            "2df3eb022736446885dc084e00ee10ee",
            "56f6b8c54a334d7bbd9790382e4d6280",
            "a5924b6828df41dbb86de20d28fe21e5",
            "68085d9395bf4bc68b7e092508079246",
            "9871ea5e8ddf428485a396e9b710fcb5",
            "7cf9ab73cefd4e72ae95c1150a045a56",
            "a13c383dfda34aa3822659180463ce72",
            "7dc9ca18eb0f483281a8257252378600",
            "80e0eae7b7e445d8856ef4510f8cb39c",
            "b3b85a006fa14f6f864587878fa29950",
            "30ba41a57589421ea787fc9b41a63b24",
            "cc5150ea22094da081a256cb693cff73",
            "9a14f6a855fd4323a68d2138d2f37c95",
            "3a9386c85aed474e96aad3ee691c394d",
            "d15d59c9d13f4134abdca1a4e5449dce",
            "59781676db3f4484a7424b775245fe83",
            "097e8edec8a44cf6b97b771c5af95162",
            "e6183e799cb94441a11c2d2049dc29c6",
            "a6c600f3cab3474baf97000ec604f37a",
            "a3d753c1ffc44f0da7b091fd985c52f1",
            "4ca757a6a84b45dbbd41fc3bb3cfbdde",
            "2de530c6121a49ca97ab4f7a1879da43",
            "265f2850068b472a894463f0a794a44f",
            "0661b35f098a4d0380e16bbb40dc3693",
            "091962f16ed64c5bae40ff4809e62a71",
            "e3a5ad5b0d9449398bcec6c29cc6cddf",
            "4d812b5707e7412b9f36fbc9ccef36a0",
            "548f7ae76dd44dfa80bab8a5b5fc0df9",
            "389a7a2aee864da5afaec1e5f78a8db7",
            "a8587dd9827747a9883ed35f6499b881",
            "6a538515eff84884b846f223df4f5eb9",
            "30328e21bf894c9fae5d04a7d646e579",
            "e78a16e514a24719aa4f32438b3d023b",
            "4eed08fff768493892287a00abb451ad"
          ]
        },
        "id": "qgjAADBFHB0S",
        "outputId": "9633f7ec-2e0f-4dba-9933-ba4b6d5ef668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a550218d4314318a7d96442049568c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model.v3:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56f6b8c54a334d7bbd9790382e4d6280"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "params.json:   0%|          | 0.00/202 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a14f6a855fd4323a68d2138d2f37c95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "consolidated.safetensors:   0%|          | 0.00/14.5G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0661b35f098a4d0380e16bbb40dc3693"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')\n",
        "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "snapshot_download(repo_id=\"mistralai/Mistral-7B-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n",
        "\n",
        "! cp -r /root/mistral_models/7B-v0.3 /content/mistral_models\n",
        "! rm -r /root/mistral_models/7B-v0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdl_R5baUyha",
        "outputId": "8ddcc9d2-5088-47a8-b5f7-d73c89063246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-24 18:50:25--  https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar\n",
            "Resolving models.mistralcdn.com (models.mistralcdn.com)... 104.26.6.117, 104.26.7.117, 172.67.70.68, ...\n",
            "Connecting to models.mistralcdn.com (models.mistralcdn.com)|104.26.6.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14496675840 (14G) [application/x-tar]\n",
            "Saving to: ‘mistral-7B-v0.3.tar’\n",
            "\n",
            "mistral-7B-v0.3.tar 100%[===================>]  13.50G  40.5MB/s    in 6m 3s   \n",
            "\n",
            "2024-05-24 18:56:29 (38.1 MB/s) - ‘mistral-7B-v0.3.tar’ saved [14496675840/14496675840]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Alternatively, you can download the model from mistral\n",
        "\n",
        "# !wget https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgJWR-fReilz"
      },
      "outputs": [],
      "source": [
        "# !DIR=/content/mistral_models && mkdir -p $DIR && tar -xf mistral-7B-v0.3.tar -C $DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PxYGmcy4gu0",
        "outputId": "71912866-1a50-4407-ac34-42e23927afd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "consolidated.safetensors  params.json  tokenizer.model.v3\n"
          ]
        }
      ],
      "source": [
        "!ls /content/mistral_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ams-19wF8zgY"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "To ensure effective training, mistral-finetune has strict requirements for how the training data has to be formatted. Check out the required data formatting [here](https://github.com/mistralai/mistral-finetune/tree/main?tab=readme-ov-file#prepare-dataset).\n",
        "\n",
        "In this example, let’s use the ultrachat_200k dataset. We load a chunk of the data into Pandas Dataframes, split the data into training and validation, and save the data into the required `jsonl` format for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# navigate to the mistral-finetune directory\n",
        "%cd /content/recode-with-mistral-finetune/example/"
      ],
      "metadata": {
        "id": "jGnb8QzGGYOf",
        "outputId": "1fb8a91a-ca60-44c7-a03f-9f569331282f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/recode-with-mistral-finetune/example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define training configuration\n",
        "# for your own use cases, you might want to change the data paths, model path, run_dir, and other hyperparameters\n",
        "\n",
        "config = \"\"\"\n",
        "# data\n",
        "data:\n",
        "  instruct_data: \"/content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl\"  # Fill\n",
        "  data: \"/content/recode-with-mistral-finetune/sample_data/train_text.jsonl\"  # Optionally fill with pretraining data\n",
        "  eval_instruct_data: \"/content/recode-with-mistral-finetune/sample_data/val_instruct.jsonl\"  # Optionally fill\n",
        "\n",
        "# model\n",
        "model_id_or_path: \"/content/mistral_models\"  # Change to downloaded path\n",
        "lora:\n",
        "  rank: 64\n",
        "\n",
        "# optim\n",
        "# tokens per training steps = batch_size x num_GPUs x seq_len\n",
        "# we recommend sequence length of 32768\n",
        "# If you run into memory error, you can try reduce the sequence length\n",
        "seq_len: 8192\n",
        "batch_size: 1\n",
        "num_microbatches: 8\n",
        "max_steps: 100\n",
        "optim:\n",
        "  lr: 1.e-4\n",
        "  weight_decay: 0.1\n",
        "  pct_start: 0.05\n",
        "\n",
        "# other\n",
        "seed: 0\n",
        "log_freq: 1\n",
        "eval_freq: 100\n",
        "no_eval: False\n",
        "ckpt_freq: 100\n",
        "\n",
        "save_adapters: True  # save only trained LoRA adapters. Set to `False` to merge LoRA adapter into the base model and save full fine-tuned model\n",
        "\n",
        "run_dir: \"/content/mistral7B_finetune_v1\"  # Fill\n",
        "\"\"\"\n",
        "\n",
        "# save the same file locally into the example.yaml file\n",
        "import yaml\n",
        "with open('mistral7B_finetune_v1.yaml', 'w') as file:\n",
        "    yaml.dump(yaml.safe_load(config), file)"
      ],
      "metadata": {
        "id": "2wYnmO7DDFKV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIK0VFXHIn8r",
        "outputId": "ba0391e3-7a0e-4df9-dbd5-3e21ab18c8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/recode-with-mistral-finetune\n"
          ]
        }
      ],
      "source": [
        "# navigate to the mistral-finetune directory\n",
        "%cd /content/recode-with-mistral-finetune/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git pull"
      ],
      "metadata": {
        "id": "jqcEWtWAIJ-8",
        "outputId": "3bc011a9-3e95-4dea-b51e-2b912ccd0cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqhyigF8XVUE",
        "outputId": "25e5ea24-55d2-49b5-c63c-f61fb7f1ecbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0it [00:00, ?it/s]Validating /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "\n",
            "\r  0% 0/44 [00:00<?, ?it/s]\u001b[A\n",
            "\r 89% 39/44 [00:00<00:00, 388.77it/s]\u001b[A\r100% 44/44 [00:00<00:00, 390.84it/s]\n",
            "\r1it [00:00,  8.80it/s]Validating /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "\n",
            "  0% 0/44 [00:00<?, ?it/s]\u001b[A\n",
            "100% 44/44 [00:00<00:00, 321.19it/s]\n",
            "2it [00:00,  7.95it/s]\n",
            "No errors! Data is correctly formatted!\n",
            "Stats for /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl and /content/recode-with-mistral-finetune/sample_data/train_text.jsonl \n",
            " -------------------- \n",
            " {\n",
            "    \"expected\": {\n",
            "        \"eta\": \"00:07:23\",\n",
            "        \"data_tokens\": 130976,\n",
            "        \"train_tokens\": 6553600,\n",
            "        \"epochs\": \"50.04\",\n",
            "        \"max_steps\": 100,\n",
            "        \"data_tokens_per_dataset\": {\n",
            "            \"/content/recode-with-mistral-finetune/sample_data/train_text.jsonl\": \"61704.0\",\n",
            "            \"/content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl\": \"69272.0\"\n",
            "        },\n",
            "        \"train_tokens_per_dataset\": {\n",
            "            \"/content/recode-with-mistral-finetune/sample_data/train_text.jsonl\": \"3276800.0\",\n",
            "            \"/content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl\": \"3276800.0\"\n",
            "        },\n",
            "        \"epochs_per_dataset\": {\n",
            "            \"/content/recode-with-mistral-finetune/sample_data/train_text.jsonl\": \"53.1\",\n",
            "            \"/content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl\": \"47.3\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "0it [00:00, ?it/s]Validating /content/recode-with-mistral-finetune/sample_data/val_instruct.jsonl ...\n",
            "\n",
            "100% 2/2 [00:00<00:00, 342.74it/s]\n",
            "1it [00:00, 162.19it/s]\n",
            "No errors! Data is correctly formatted!\n"
          ]
        }
      ],
      "source": [
        "# Now you can verify your training yaml to make sure the data is correctly formatted and to get an estimate of your training time.\n",
        "\n",
        "!python -m utils.validate_data --train_yaml example/mistral7B_finetune_v1.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hia7n0T1_mHZ"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZtcLerooWFeB"
      },
      "outputs": [],
      "source": [
        "# these info is needed for training\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ErD1ktQUMyPZ"
      },
      "outputs": [],
      "source": [
        "# make sure the run_dir has not been created before\n",
        "# only run this when you ran torchrun previously and created the /content/mistral7B_finetune_v1\n",
        "# ! rm -r /content/mistral7B_finetune_v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4wFgmwIUTtg",
        "outputId": "e0fa61ae-930c-4b13-b51b-1f10747ec426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-30 15:21:12.266811: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-30 15:21:12.283917: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-30 15:21:12.304891: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-30 15:21:12.311255: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-30 15:21:12.326173: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-30 15:21:13.478987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "args: TrainArgs(data=DataArgs(data='/content/recode-with-mistral-finetune/sample_data/train_text.jsonl', shuffle=False, instruct_data='/content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl', eval_instruct_data='/content/recode-with-mistral-finetune/sample_data/val_instruct.jsonl', instruct=InstructArgs(shuffle=True, dynamic_chunk_fn_call=True)), model_id_or_path='/content/mistral_models', run_dir='/content/mistral7B_finetune_v1', optim=OptimArgs(lr=0.0001, weight_decay=0.1, pct_start=0.05), seed=0, num_microbatches=8, seq_len=8192, batch_size=1, max_norm=1.0, max_steps=100, log_freq=1, ckpt_freq=100, save_adapters=True, no_ckpt=False, num_ckpt_keep=3, eval_freq=100, no_eval=False, checkpoint=True, world_size=1, wandb=WandbArgs(project=None, offline=False, key=None, run_name=None), mlflow=MLFlowArgs(tracking_uri=None, experiment_name=None), lora=LoraArgs(enable=True, rank=64, dropout=0.0, scaling=2.0))\n",
            "2024-12-30 15:21:14 (UTC) - 0:00:05 - distributed - INFO - torch.cuda.device_count: 1\n",
            "2024-12-30 15:21:14 (UTC) - 0:00:05 - distributed - INFO - CUDA_VISIBLE_DEVICES: 0\n",
            "2024-12-30 15:21:14 (UTC) - 0:00:05 - distributed - INFO - local rank: 0\n",
            "2024-12-30 15:21:14 (UTC) - 0:00:05 - train - INFO - Going to init comms...\n",
            "2024-12-30 15:21:14 (UTC) - 0:00:05 - train - INFO - Run dir: /content/mistral7B_finetune_v1\n",
            "2024-12-30 15:21:15 (UTC) - 0:00:06 - train - INFO - TrainArgs: {'batch_size': 1,\n",
            " 'checkpoint': True,\n",
            " 'ckpt_freq': 100,\n",
            " 'data': {'data': '/content/recode-with-mistral-finetune/sample_data/train_text.jsonl',\n",
            "          'eval_instruct_data': '/content/recode-with-mistral-finetune/sample_data/val_instruct.jsonl',\n",
            "          'instruct': {'dynamic_chunk_fn_call': True, 'shuffle': True},\n",
            "          'instruct_data': '/content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl',\n",
            "          'shuffle': False},\n",
            " 'eval_freq': 100,\n",
            " 'log_freq': 1,\n",
            " 'lora': {'dropout': 0.0, 'enable': True, 'rank': 64, 'scaling': 2.0},\n",
            " 'max_norm': 1.0,\n",
            " 'max_steps': 100,\n",
            " 'mlflow': {'experiment_name': None, 'tracking_uri': None},\n",
            " 'model_id_or_path': '/content/mistral_models',\n",
            " 'no_ckpt': False,\n",
            " 'no_eval': False,\n",
            " 'num_ckpt_keep': 3,\n",
            " 'num_microbatches': 8,\n",
            " 'optim': {'lr': 0.0001, 'pct_start': 0.05, 'weight_decay': 0.1},\n",
            " 'run_dir': '/content/mistral7B_finetune_v1',\n",
            " 'save_adapters': True,\n",
            " 'seed': 0,\n",
            " 'seq_len': 8192,\n",
            " 'wandb': {'key': None, 'offline': False, 'project': None, 'run_name': None},\n",
            " 'world_size': 1}\n",
            "2024-12-30 15:21:15 (UTC) - 0:00:06 - finetune.wrapped_model - INFO - Reloading model from /content/mistral_models/consolidated.safetensors ...\n",
            "2024-12-30 15:21:15 (UTC) - 0:00:06 - finetune.wrapped_model - INFO - Converting model to dtype torch.bfloat16 ...\n",
            "2024-12-30 15:21:15 (UTC) - 0:00:06 - finetune.wrapped_model - INFO - Loaded model on cpu!\n",
            "2024-12-30 15:21:15 (UTC) - 0:00:06 - finetune.wrapped_model - INFO - Initializing lora layers ...\n",
            "2024-12-30 15:21:16 (UTC) - 0:00:07 - finetune.wrapped_model - INFO - Finished initialization!\n",
            "2024-12-30 15:21:16 (UTC) - 0:00:07 - finetune.wrapped_model - INFO - Sharding model over 1 GPUs ...\n",
            "2024-12-30 15:21:20 (UTC) - 0:00:12 - finetune.wrapped_model - INFO - Model sharded!\n",
            "2024-12-30 15:21:21 (UTC) - 0:00:12 - finetune.wrapped_model - INFO - 167,772,160 out of 7,415,795,712 parameters are finetuned (2.26%).\n",
            "2024-12-30 15:21:21 (UTC) - 0:00:12 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:21:33 (UTC) - 0:00:24 - dataset - INFO - Loading /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:21:33 (UTC) - 0:00:24 - dataset - INFO - /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl loaded and tokenized.\n",
            "2024-12-30 15:21:33 (UTC) - 0:00:24 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:21:40 (UTC) - 0:00:31 - train - INFO - step: 000001 - done (%): 1.0 - loss: 1.048 - lr: 4.0e-06 - peak_alloc_mem (GB): 21.0 - alloc_mem (GB): 17.1 - words_per_second: 3423.6 - avg_words_per_second: 3423.6 - ETA: >2024-12-30 15:53:15\n",
            "2024-12-30 15:21:49 (UTC) - 0:00:40 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:21:53 (UTC) - 0:00:45 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:21:58 (UTC) - 0:00:49 - train - INFO - step: 000002 - done (%): 2.0 - loss: 1.381 - lr: 1.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3691.4 - avg_words_per_second: 3552.5 - ETA: >2024-12-30 15:52:06\n",
            "2024-12-30 15:22:07 (UTC) - 0:00:58 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:22:16 (UTC) - 0:01:07 - train - INFO - step: 000003 - done (%): 3.0 - loss: 1.165 - lr: 5.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3656.5 - avg_words_per_second: 3586.5 - ETA: >2024-12-30 15:51:48\n",
            "2024-12-30 15:22:27 (UTC) - 0:01:18 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:22:34 (UTC) - 0:01:25 - train - INFO - step: 000004 - done (%): 4.0 - loss: 0.724 - lr: 8.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3641.1 - avg_words_per_second: 3600.0 - ETA: >2024-12-30 15:51:41\n",
            "2024-12-30 15:22:49 (UTC) - 0:01:41 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:22:52 (UTC) - 0:01:43 - train - INFO - step: 000005 - done (%): 5.0 - loss: 0.673 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3676.2 - avg_words_per_second: 3615.0 - ETA: >2024-12-30 15:51:34\n",
            "2024-12-30 15:22:54 (UTC) - 0:01:45 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:23:10 (UTC) - 0:02:01 - train - INFO - step: 000006 - done (%): 6.0 - loss: 0.421 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3641.6 - avg_words_per_second: 3619.4 - ETA: >2024-12-30 15:51:32\n",
            "2024-12-30 15:23:16 (UTC) - 0:02:08 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:23:28 (UTC) - 0:02:19 - train - INFO - step: 000007 - done (%): 7.0 - loss: 0.524 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3640.5 - avg_words_per_second: 3622.4 - ETA: >2024-12-30 15:51:30\n",
            "2024-12-30 15:23:39 (UTC) - 0:02:30 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:23:45 (UTC) - 0:02:37 - train - INFO - step: 000008 - done (%): 8.0 - loss: 0.312 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3677.5 - avg_words_per_second: 3629.2 - ETA: >2024-12-30 15:51:27\n",
            "2024-12-30 15:23:52 (UTC) - 0:02:43 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:24:01 (UTC) - 0:02:52 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:24:03 (UTC) - 0:02:55 - train - INFO - step: 000009 - done (%): 9.0 - loss: 0.281 - lr: 1.0e-04 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3659.3 - avg_words_per_second: 3632.5 - ETA: >2024-12-30 15:51:25\n",
            "2024-12-30 15:24:21 (UTC) - 0:03:13 - train - INFO - step: 000010 - done (%): 10.0 - loss: 0.240 - lr: 9.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3665.1 - avg_words_per_second: 3635.7 - ETA: >2024-12-30 15:51:24\n",
            "2024-12-30 15:24:28 (UTC) - 0:03:19 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:24:32 (UTC) - 0:03:24 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:24:39 (UTC) - 0:03:30 - train - INFO - step: 000011 - done (%): 11.0 - loss: 0.162 - lr: 9.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3660.5 - avg_words_per_second: 3638.0 - ETA: >2024-12-30 15:51:22\n",
            "2024-12-30 15:24:52 (UTC) - 0:03:44 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:24:57 (UTC) - 0:03:48 - train - INFO - step: 000012 - done (%): 12.0 - loss: 0.172 - lr: 9.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3679.1 - avg_words_per_second: 3641.4 - ETA: >2024-12-30 15:51:21\n",
            "2024-12-30 15:25:15 (UTC) - 0:04:06 - train - INFO - step: 000013 - done (%): 13.0 - loss: 0.154 - lr: 9.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3673.1 - avg_words_per_second: 3643.8 - ETA: >2024-12-30 15:51:20\n",
            "2024-12-30 15:25:15 (UTC) - 0:04:06 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:25:24 (UTC) - 0:04:15 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:25:33 (UTC) - 0:04:24 - train - INFO - step: 000014 - done (%): 14.0 - loss: 0.193 - lr: 9.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3634.0 - avg_words_per_second: 3643.1 - ETA: >2024-12-30 15:51:20\n",
            "2024-12-30 15:25:42 (UTC) - 0:04:33 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:25:51 (UTC) - 0:04:42 - train - INFO - step: 000015 - done (%): 15.0 - loss: 0.138 - lr: 9.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3667.6 - avg_words_per_second: 3644.7 - ETA: >2024-12-30 15:51:19\n",
            "2024-12-30 15:26:00 (UTC) - 0:04:51 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:26:09 (UTC) - 0:05:00 - train - INFO - step: 000016 - done (%): 16.0 - loss: 0.129 - lr: 9.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3642.5 - avg_words_per_second: 3644.6 - ETA: >2024-12-30 15:51:19\n",
            "2024-12-30 15:26:11 (UTC) - 0:05:02 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:26:27 (UTC) - 0:05:18 - train - INFO - step: 000017 - done (%): 17.0 - loss: 0.128 - lr: 9.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3672.4 - avg_words_per_second: 3646.2 - ETA: >2024-12-30 15:51:18\n",
            "2024-12-30 15:26:27 (UTC) - 0:05:18 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:26:44 (UTC) - 0:05:36 - train - INFO - step: 000018 - done (%): 18.0 - loss: 0.070 - lr: 9.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3648.0 - avg_words_per_second: 3646.3 - ETA: >2024-12-30 15:51:18\n",
            "2024-12-30 15:26:44 (UTC) - 0:05:36 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:27:02 (UTC) - 0:05:54 - train - INFO - step: 000019 - done (%): 19.0 - loss: 0.083 - lr: 9.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3682.9 - avg_words_per_second: 3648.2 - ETA: >2024-12-30 15:51:17\n",
            "2024-12-30 15:27:02 (UTC) - 0:05:54 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:27:07 (UTC) - 0:05:58 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:27:20 (UTC) - 0:06:12 - train - INFO - step: 000020 - done (%): 20.0 - loss: 0.102 - lr: 9.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3645.9 - avg_words_per_second: 3648.1 - ETA: >2024-12-30 15:51:17\n",
            "2024-12-30 15:27:31 (UTC) - 0:06:23 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:27:38 (UTC) - 0:06:29 - train - INFO - step: 000021 - done (%): 21.0 - loss: 0.059 - lr: 9.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3670.3 - avg_words_per_second: 3649.1 - ETA: >2024-12-30 15:51:17\n",
            "2024-12-30 15:27:49 (UTC) - 0:06:41 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:27:56 (UTC) - 0:06:47 - train - INFO - step: 000022 - done (%): 22.0 - loss: 0.074 - lr: 9.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3647.6 - avg_words_per_second: 3649.1 - ETA: >2024-12-30 15:51:17\n",
            "2024-12-30 15:27:56 (UTC) - 0:06:47 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:28:14 (UTC) - 0:07:05 - train - INFO - step: 000023 - done (%): 23.0 - loss: 0.064 - lr: 9.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3670.7 - avg_words_per_second: 3650.0 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:28:16 (UTC) - 0:07:07 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:28:27 (UTC) - 0:07:19 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:28:32 (UTC) - 0:07:23 - train - INFO - step: 000024 - done (%): 24.0 - loss: 0.069 - lr: 9.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3675.2 - avg_words_per_second: 3651.0 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:28:50 (UTC) - 0:07:41 - train - INFO - step: 000025 - done (%): 25.0 - loss: 0.072 - lr: 8.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3650.6 - avg_words_per_second: 3651.0 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:28:52 (UTC) - 0:07:43 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:28:59 (UTC) - 0:07:50 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:29:08 (UTC) - 0:07:59 - train - INFO - step: 000026 - done (%): 26.0 - loss: 0.056 - lr: 8.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3669.2 - avg_words_per_second: 3651.7 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:29:17 (UTC) - 0:08:08 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:29:25 (UTC) - 0:08:17 - train - INFO - step: 000027 - done (%): 27.0 - loss: 0.056 - lr: 8.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3665.9 - avg_words_per_second: 3652.2 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:29:41 (UTC) - 0:08:33 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:29:43 (UTC) - 0:08:35 - train - INFO - step: 000028 - done (%): 28.0 - loss: 0.053 - lr: 8.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3636.4 - avg_words_per_second: 3651.7 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:29:43 (UTC) - 0:08:35 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:30:01 (UTC) - 0:08:53 - train - INFO - step: 000029 - done (%): 29.0 - loss: 0.048 - lr: 8.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3674.3 - avg_words_per_second: 3652.5 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:30:04 (UTC) - 0:08:55 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:30:17 (UTC) - 0:09:08 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:30:19 (UTC) - 0:09:11 - train - INFO - step: 000030 - done (%): 30.0 - loss: 0.050 - lr: 8.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3642.4 - avg_words_per_second: 3652.1 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:30:37 (UTC) - 0:09:28 - train - INFO - step: 000031 - done (%): 31.0 - loss: 0.040 - lr: 8.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3670.6 - avg_words_per_second: 3652.7 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:30:37 (UTC) - 0:09:28 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:30:53 (UTC) - 0:09:44 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:30:55 (UTC) - 0:09:46 - train - INFO - step: 000032 - done (%): 32.0 - loss: 0.053 - lr: 8.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3644.9 - avg_words_per_second: 3652.5 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:31:02 (UTC) - 0:09:53 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:31:13 (UTC) - 0:10:04 - train - INFO - step: 000033 - done (%): 33.0 - loss: 0.043 - lr: 8.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3670.3 - avg_words_per_second: 3653.0 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:31:20 (UTC) - 0:10:11 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:31:31 (UTC) - 0:10:22 - train - INFO - step: 000034 - done (%): 34.0 - loss: 0.042 - lr: 7.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3642.0 - avg_words_per_second: 3652.7 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:31:47 (UTC) - 0:10:38 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:31:49 (UTC) - 0:10:40 - train - INFO - step: 000035 - done (%): 35.0 - loss: 0.036 - lr: 7.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3641.9 - avg_words_per_second: 3652.4 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:31:51 (UTC) - 0:10:43 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:32:07 (UTC) - 0:10:58 - train - INFO - step: 000036 - done (%): 36.0 - loss: 0.037 - lr: 7.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3618.5 - avg_words_per_second: 3651.4 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:32:12 (UTC) - 0:11:03 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:32:25 (UTC) - 0:11:16 - train - INFO - step: 000037 - done (%): 37.0 - loss: 0.027 - lr: 7.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3674.1 - avg_words_per_second: 3652.0 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:32:34 (UTC) - 0:11:25 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:32:38 (UTC) - 0:11:30 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:32:43 (UTC) - 0:11:34 - train - INFO - step: 000038 - done (%): 38.0 - loss: 0.032 - lr: 7.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3638.3 - avg_words_per_second: 3651.7 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:32:54 (UTC) - 0:11:45 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:33:01 (UTC) - 0:11:52 - train - INFO - step: 000039 - done (%): 39.0 - loss: 0.033 - lr: 7.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3638.3 - avg_words_per_second: 3651.3 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:33:12 (UTC) - 0:12:03 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:33:19 (UTC) - 0:12:10 - train - INFO - step: 000040 - done (%): 40.0 - loss: 0.031 - lr: 7.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3671.1 - avg_words_per_second: 3651.8 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:33:35 (UTC) - 0:12:26 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:33:37 (UTC) - 0:12:28 - train - INFO - step: 000041 - done (%): 41.0 - loss: 0.033 - lr: 6.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3645.7 - avg_words_per_second: 3651.7 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:33:50 (UTC) - 0:12:42 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:33:55 (UTC) - 0:12:46 - train - INFO - step: 000042 - done (%): 42.0 - loss: 0.023 - lr: 6.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3655.7 - avg_words_per_second: 3651.8 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:34:01 (UTC) - 0:12:53 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:34:13 (UTC) - 0:13:04 - train - INFO - step: 000043 - done (%): 43.0 - loss: 0.035 - lr: 6.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3648.7 - avg_words_per_second: 3651.7 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:34:24 (UTC) - 0:13:15 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:34:31 (UTC) - 0:13:22 - train - INFO - step: 000044 - done (%): 44.0 - loss: 0.028 - lr: 6.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3664.9 - avg_words_per_second: 3652.0 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:34:35 (UTC) - 0:13:26 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:34:48 (UTC) - 0:13:40 - train - INFO - step: 000045 - done (%): 45.0 - loss: 0.028 - lr: 6.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3672.0 - avg_words_per_second: 3652.4 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:34:55 (UTC) - 0:13:46 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:35:07 (UTC) - 0:13:58 - train - INFO - step: 000046 - done (%): 46.0 - loss: 0.025 - lr: 6.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3627.5 - avg_words_per_second: 3651.9 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:35:15 (UTC) - 0:14:07 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:35:20 (UTC) - 0:14:11 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:35:24 (UTC) - 0:14:16 - train - INFO - step: 000047 - done (%): 47.0 - loss: 0.027 - lr: 5.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3667.1 - avg_words_per_second: 3652.2 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:35:42 (UTC) - 0:14:34 - train - INFO - step: 000048 - done (%): 48.0 - loss: 0.026 - lr: 5.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3642.9 - avg_words_per_second: 3652.0 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:35:42 (UTC) - 0:14:34 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:36:00 (UTC) - 0:14:52 - train - INFO - step: 000049 - done (%): 49.0 - loss: 0.021 - lr: 5.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3623.7 - avg_words_per_second: 3651.4 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:36:00 (UTC) - 0:14:52 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:36:18 (UTC) - 0:15:10 - train - INFO - step: 000050 - done (%): 50.0 - loss: 0.024 - lr: 5.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3645.5 - avg_words_per_second: 3651.3 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:36:21 (UTC) - 0:15:12 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:36:25 (UTC) - 0:15:16 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:36:36 (UTC) - 0:15:28 - train - INFO - step: 000051 - done (%): 51.0 - loss: 0.023 - lr: 5.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3664.2 - avg_words_per_second: 3651.6 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:36:39 (UTC) - 0:15:30 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:36:54 (UTC) - 0:15:46 - train - INFO - step: 000052 - done (%): 52.0 - loss: 0.023 - lr: 5.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3642.7 - avg_words_per_second: 3651.4 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:37:08 (UTC) - 0:15:59 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:37:12 (UTC) - 0:16:04 - train - INFO - step: 000053 - done (%): 53.0 - loss: 0.025 - lr: 4.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3624.8 - avg_words_per_second: 3650.9 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:37:19 (UTC) - 0:16:10 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:37:30 (UTC) - 0:16:22 - train - INFO - step: 000054 - done (%): 54.0 - loss: 0.024 - lr: 4.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3672.7 - avg_words_per_second: 3651.3 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:37:37 (UTC) - 0:16:28 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:37:44 (UTC) - 0:16:35 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:37:48 (UTC) - 0:16:39 - train - INFO - step: 000055 - done (%): 55.0 - loss: 0.017 - lr: 4.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3672.0 - avg_words_per_second: 3651.7 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:38:06 (UTC) - 0:16:57 - train - INFO - step: 000056 - done (%): 56.0 - loss: 0.026 - lr: 4.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3658.4 - avg_words_per_second: 3651.8 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:38:06 (UTC) - 0:16:57 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:38:22 (UTC) - 0:17:13 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:38:24 (UTC) - 0:17:15 - train - INFO - step: 000057 - done (%): 57.0 - loss: 0.018 - lr: 4.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3658.2 - avg_words_per_second: 3651.9 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:38:42 (UTC) - 0:17:33 - train - INFO - step: 000058 - done (%): 58.0 - loss: 0.021 - lr: 4.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3664.7 - avg_words_per_second: 3652.1 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:38:44 (UTC) - 0:17:35 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:38:46 (UTC) - 0:17:38 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:39:00 (UTC) - 0:17:51 - train - INFO - step: 000059 - done (%): 59.0 - loss: 0.018 - lr: 3.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3625.8 - avg_words_per_second: 3651.7 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:39:13 (UTC) - 0:18:05 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:39:18 (UTC) - 0:18:09 - train - INFO - step: 000060 - done (%): 60.0 - loss: 0.027 - lr: 3.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3663.2 - avg_words_per_second: 3651.9 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:39:31 (UTC) - 0:18:22 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:39:36 (UTC) - 0:18:27 - train - INFO - step: 000061 - done (%): 61.0 - loss: 0.027 - lr: 3.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3683.1 - avg_words_per_second: 3652.4 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:39:42 (UTC) - 0:18:34 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:39:49 (UTC) - 0:18:40 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:39:54 (UTC) - 0:18:45 - train - INFO - step: 000062 - done (%): 62.0 - loss: 0.026 - lr: 3.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3638.3 - avg_words_per_second: 3652.1 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:40:11 (UTC) - 0:19:03 - train - INFO - step: 000063 - done (%): 63.0 - loss: 0.018 - lr: 3.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3663.1 - avg_words_per_second: 3652.3 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:40:12 (UTC) - 0:19:03 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:40:29 (UTC) - 0:19:21 - train - INFO - step: 000064 - done (%): 64.0 - loss: 0.023 - lr: 3.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3649.6 - avg_words_per_second: 3652.3 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:40:34 (UTC) - 0:19:25 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:40:41 (UTC) - 0:19:32 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:40:47 (UTC) - 0:19:39 - train - INFO - step: 000065 - done (%): 65.0 - loss: 0.021 - lr: 3.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3664.8 - avg_words_per_second: 3652.5 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:41:01 (UTC) - 0:19:52 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:41:05 (UTC) - 0:19:56 - train - INFO - step: 000066 - done (%): 66.0 - loss: 0.018 - lr: 2.8e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3675.3 - avg_words_per_second: 3652.8 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:41:19 (UTC) - 0:20:10 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:41:23 (UTC) - 0:20:14 - train - INFO - step: 000067 - done (%): 67.0 - loss: 0.023 - lr: 2.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3641.6 - avg_words_per_second: 3652.6 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:41:30 (UTC) - 0:20:21 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:41:41 (UTC) - 0:20:32 - train - INFO - step: 000068 - done (%): 68.0 - loss: 0.019 - lr: 2.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3668.5 - avg_words_per_second: 3652.9 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:41:50 (UTC) - 0:20:41 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:41:59 (UTC) - 0:20:50 - train - INFO - step: 000069 - done (%): 69.0 - loss: 0.021 - lr: 2.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3641.9 - avg_words_per_second: 3652.7 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:42:08 (UTC) - 0:20:59 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:42:10 (UTC) - 0:21:01 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:42:17 (UTC) - 0:21:08 - train - INFO - step: 000070 - done (%): 70.0 - loss: 0.018 - lr: 2.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3674.5 - avg_words_per_second: 3653.0 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:42:35 (UTC) - 0:21:26 - train - INFO - step: 000071 - done (%): 71.0 - loss: 0.019 - lr: 2.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3639.0 - avg_words_per_second: 3652.8 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:42:37 (UTC) - 0:21:28 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:42:46 (UTC) - 0:21:37 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:42:53 (UTC) - 0:21:44 - train - INFO - step: 000072 - done (%): 72.0 - loss: 0.015 - lr: 2.0e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3671.8 - avg_words_per_second: 3653.1 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:43:11 (UTC) - 0:22:02 - train - INFO - step: 000073 - done (%): 73.0 - loss: 0.017 - lr: 1.9e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3650.6 - avg_words_per_second: 3653.0 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:43:13 (UTC) - 0:22:04 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:43:15 (UTC) - 0:22:06 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:43:29 (UTC) - 0:22:20 - train - INFO - step: 000074 - done (%): 74.0 - loss: 0.017 - lr: 1.7e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3637.8 - avg_words_per_second: 3652.8 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:43:38 (UTC) - 0:22:29 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:43:47 (UTC) - 0:22:38 - train - INFO - step: 000075 - done (%): 75.0 - loss: 0.021 - lr: 1.6e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3632.8 - avg_words_per_second: 3652.6 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:43:53 (UTC) - 0:22:45 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:44:05 (UTC) - 0:22:56 - train - INFO - step: 000076 - done (%): 76.0 - loss: 0.016 - lr: 1.5e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3665.1 - avg_words_per_second: 3652.7 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:44:09 (UTC) - 0:23:00 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:44:20 (UTC) - 0:23:12 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:44:23 (UTC) - 0:23:14 - train - INFO - step: 000077 - done (%): 77.0 - loss: 0.018 - lr: 1.4e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3634.0 - avg_words_per_second: 3652.5 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:44:41 (UTC) - 0:23:32 - train - INFO - step: 000078 - done (%): 78.0 - loss: 0.017 - lr: 1.3e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3646.6 - avg_words_per_second: 3652.4 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:44:41 (UTC) - 0:23:32 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:44:54 (UTC) - 0:23:45 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:44:58 (UTC) - 0:23:50 - train - INFO - step: 000079 - done (%): 79.0 - loss: 0.019 - lr: 1.2e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3680.8 - avg_words_per_second: 3652.8 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:45:16 (UTC) - 0:24:08 - train - INFO - step: 000080 - done (%): 80.0 - loss: 0.016 - lr: 1.1e-05 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3651.6 - avg_words_per_second: 3652.8 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:45:16 (UTC) - 0:24:08 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:45:23 (UTC) - 0:24:14 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:45:34 (UTC) - 0:24:26 - train - INFO - step: 000081 - done (%): 81.0 - loss: 0.017 - lr: 9.5e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3664.6 - avg_words_per_second: 3652.9 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:45:43 (UTC) - 0:24:35 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:45:52 (UTC) - 0:24:44 - train - INFO - step: 000082 - done (%): 82.0 - loss: 0.013 - lr: 8.6e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3624.0 - avg_words_per_second: 3652.5 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:46:06 (UTC) - 0:24:57 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:46:08 (UTC) - 0:24:59 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:46:10 (UTC) - 0:25:02 - train - INFO - step: 000083 - done (%): 83.0 - loss: 0.016 - lr: 7.7e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3665.9 - avg_words_per_second: 3652.7 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:46:28 (UTC) - 0:25:19 - train - INFO - step: 000084 - done (%): 84.0 - loss: 0.014 - lr: 6.8e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3668.0 - avg_words_per_second: 3652.9 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:46:35 (UTC) - 0:25:26 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:46:39 (UTC) - 0:25:31 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:46:46 (UTC) - 0:25:37 - train - INFO - step: 000085 - done (%): 85.0 - loss: 0.016 - lr: 6.0e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3661.8 - avg_words_per_second: 3653.0 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:47:04 (UTC) - 0:25:55 - train - INFO - step: 000086 - done (%): 86.0 - loss: 0.018 - lr: 5.3e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3671.9 - avg_words_per_second: 3653.2 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:47:06 (UTC) - 0:25:57 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:47:22 (UTC) - 0:26:13 - train - INFO - step: 000087 - done (%): 87.0 - loss: 0.016 - lr: 4.6e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3673.7 - avg_words_per_second: 3653.4 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:47:24 (UTC) - 0:26:15 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:47:33 (UTC) - 0:26:24 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:47:40 (UTC) - 0:26:31 - train - INFO - step: 000088 - done (%): 88.0 - loss: 0.020 - lr: 3.9e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3639.9 - avg_words_per_second: 3653.3 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:47:53 (UTC) - 0:26:44 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:47:58 (UTC) - 0:26:49 - train - INFO - step: 000089 - done (%): 89.0 - loss: 0.016 - lr: 3.3e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3625.0 - avg_words_per_second: 3653.0 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:48:00 (UTC) - 0:26:51 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:48:16 (UTC) - 0:27:07 - train - INFO - step: 000090 - done (%): 90.0 - loss: 0.016 - lr: 2.7e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3676.0 - avg_words_per_second: 3653.2 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:48:27 (UTC) - 0:27:18 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:48:34 (UTC) - 0:27:25 - train - INFO - step: 000091 - done (%): 91.0 - loss: 0.015 - lr: 2.2e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3627.5 - avg_words_per_second: 3652.9 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:48:34 (UTC) - 0:27:25 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:48:49 (UTC) - 0:27:41 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:48:52 (UTC) - 0:27:43 - train - INFO - step: 000092 - done (%): 92.0 - loss: 0.018 - lr: 1.7e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3646.7 - avg_words_per_second: 3652.9 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:49:10 (UTC) - 0:28:01 - train - INFO - step: 000093 - done (%): 93.0 - loss: 0.014 - lr: 1.3e-06 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3668.6 - avg_words_per_second: 3653.0 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:49:12 (UTC) - 0:28:03 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:49:27 (UTC) - 0:28:19 - train - INFO - step: 000094 - done (%): 94.0 - loss: 0.013 - lr: 9.8e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3663.1 - avg_words_per_second: 3653.1 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:49:32 (UTC) - 0:28:23 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:49:45 (UTC) - 0:28:37 - train - INFO - step: 000095 - done (%): 95.0 - loss: 0.020 - lr: 6.8e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3668.5 - avg_words_per_second: 3653.3 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:49:45 (UTC) - 0:28:37 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:49:56 (UTC) - 0:28:48 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:50:03 (UTC) - 0:28:55 - train - INFO - step: 000096 - done (%): 96.0 - loss: 0.018 - lr: 4.4e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3631.9 - avg_words_per_second: 3653.1 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:50:19 (UTC) - 0:29:10 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:50:21 (UTC) - 0:29:12 - train - INFO - step: 000097 - done (%): 97.0 - loss: 0.014 - lr: 2.5e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3668.5 - avg_words_per_second: 3653.2 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:50:23 (UTC) - 0:29:15 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:50:39 (UTC) - 0:29:30 - train - INFO - step: 000098 - done (%): 98.0 - loss: 0.017 - lr: 1.1e-07 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3666.0 - avg_words_per_second: 3653.4 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:50:46 (UTC) - 0:29:37 - dataset - INFO - Shuffling /content/recode-with-mistral-finetune/sample_data/train_instruct.jsonl ...\n",
            "2024-12-30 15:50:52 (UTC) - 0:29:44 - dataset - INFO - Lazily loading /content/recode-with-mistral-finetune/sample_data/train_text.jsonl ...\n",
            "2024-12-30 15:50:57 (UTC) - 0:29:48 - train - INFO - step: 000099 - done (%): 99.0 - loss: 0.017 - lr: 2.8e-08 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3657.0 - avg_words_per_second: 3653.4 - ETA: >2024-12-30 15:51:15\n",
            "2024-12-30 15:51:15 (UTC) - 0:30:06 - eval - INFO - Start eval...\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:07 - eval - INFO - Eval finished!\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:07 - train - INFO - step: 000100 - eval_perplexity: 1.166 - eval_loss: 0.222 - train_loss: 0.013\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:07 - train - INFO - step: 000100 - done (%): 100.0 - loss: 0.013 - lr: 4.0e-10 - peak_alloc_mem (GB): 22.2 - alloc_mem (GB): 17.1 - words_per_second: 3522.7 - avg_words_per_second: 3652.1 - ETA: >2024-12-30 15:51:16\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:07 - checkpointing - INFO - Dumping checkpoint in /content/mistral7B_finetune_v1/checkpoints/checkpoint_000100/consolidated using tmp name: tmp.consolidated\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:08 - checkpointing - INFO - Done dumping checkpoint in /content/mistral7B_finetune_v1/checkpoints/checkpoint_000100/consolidated for step: 100\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:08 - checkpointing - INFO - Done deleting checkpoints \n",
            "2024-12-30 15:51:16 (UTC) - 0:30:08 - checkpointing - INFO - Done!\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:08 - train - INFO - done!\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:08 - utils - INFO - Closing: eval_logger\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:08 - utils - INFO - Closed: eval_logger\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:08 - utils - INFO - Closing: metrics_logger\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:08 - utils - INFO - Closed: metrics_logger\n",
            "2024-12-30 15:51:16 (UTC) - 0:30:08 - train - INFO - Closed everything!\n"
          ]
        }
      ],
      "source": [
        "# start training\n",
        "\n",
        "!torchrun --nproc-per-node 1 -m train example/mistral7B_finetune_v1.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruJ29JFn98zE"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BWNGKt9-Kxz",
        "outputId": "70035172-b8ea-42c1-8a1e-d6e5b3da5fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mistral_inference\n",
            "  Downloading mistral_inference-1.5.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: fire>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: mistral_common>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (1.5.1)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (10.4.0)\n",
            "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.4.5)\n",
            "Requirement already satisfied: simple-parsing>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.1.6)\n",
            "Requirement already satisfied: xformers>=0.0.24 in /usr/local/lib/python3.10/dist-packages (from mistral_inference) (0.0.24)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.6.0->mistral_inference) (2.5.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (2.10.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (0.2.0)\n",
            "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from mistral_common>=1.4.0->mistral_inference) (4.12.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing>=0.1.5->mistral_inference) (0.16)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (from xformers>=0.0.24->mistral_inference) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers>=0.0.24->mistral_inference) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->xformers>=0.0.24->mistral_inference) (12.6.85)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral_common>=1.4.0->mistral_inference) (0.22.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.1->mistral_common>=1.4.0->mistral_inference) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->mistral_common>=1.4.0->mistral_inference) (2024.12.14)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->mistral_common>=1.4.0->mistral_inference) (2024.11.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->xformers>=0.0.24->mistral_inference) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->xformers>=0.0.24->mistral_inference) (1.3.0)\n",
            "Downloading mistral_inference-1.5.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: mistral_inference\n",
            "Successfully installed mistral_inference-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mistral_inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba"
      ],
      "metadata": {
        "id": "GtcME24yWCoQ",
        "outputId": "4ed9e270-0c9b-4cdf-c7ee-9c0b64a8f491",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "cuda.select_device(0)\n",
        "cuda.close()"
      ],
      "metadata": {
        "id": "jG3rQC6QVlmq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda.select_device(0)\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "Tz-kI3EAWRdQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "-_jhdyRWTMHq",
        "outputId": "4fce7680-bde1-41ba-f1d0-9caaf683a2c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-66a34127-4b30-7bc5-0dcc-b8f445ef8962)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: libérer la mémoire gpu avec google collab\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Release GPU memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Optionally, you can also try to delete large objects you no longer need\n",
        "# Replace 'your_large_variable' with the actual name of your variable\n",
        "# del your_large_variable\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# Check GPU memory usage again\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Q-2xn3tJTVPx",
        "outputId": "5f4bf9f6-7ee0-4ebc-a8d9-1259f7d7289d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 30 16:17:12 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0              42W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
      ],
      "metadata": {
        "id": "wKssAAsnVK8x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "metadata": {
        "id": "CQjBmgaWV0BQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F-xLs2Ot9-il"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "from mistral_inference.transformer import Transformer\n",
        "from mistral_inference.generate import generate\n",
        "\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "from mistral_common.protocol.instruct.messages import UserMessage, SystemMessage\n",
        "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
        "\n",
        "finetune_model_name = \"instruct_icd_v1\"\n",
        "source_model_path = \"/content/mistral_models\"\n",
        "finetune_model_path = \"/content/mistral7B_finetune_v1\"\n",
        "data_path = \"/content/recode-with-mistral-finetune/sample_data/test_instruct.jsonl\"\n",
        "\n",
        "tokenizer = MistralTokenizer.from_file(source_model_path+\"/tokenizer.model.v3\")  # change to extracted tokenizer file\n",
        "model = Transformer.from_folder(source_model_path)  # change to extracted model dir\n",
        "model.load_lora(finetune_model_path+\"/checkpoints/checkpoint_000100/consolidated/lora.safetensors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vd8A8JP4Fx3C"
      },
      "outputs": [],
      "source": [
        "def pred_codes_stat(data,tokenizer,model):\n",
        "    system_message = SystemMessage(\n",
        "        content=\"Vous êtes un modèle de langage en française spécialisé dans le codage des diagnostics selon la classification internationale des maladies version 10 (CIM-10) pour les résumés standardisés de sortie du programme de médicalisation des systèmes d'information français (PMSI). A partir des comptes rendus d'hospitalisation vous donnerez les codes diagnostics CIM-10 que l'on peut retenir pour le séjours en distiguant diagnostic principal, diagnostic relié et diagnostics associés.\")\n",
        "    user_message = UserMessage(\n",
        "      content=\"Générez les codes CIM et leurs définitions pour le résumé du séjour suivant : {TEXT}\".format(TEXT = data[\"messages\"][1][\"content\"]))\n",
        "    completion_request = ChatCompletionRequest(messages=[system_message , user_message])\n",
        "\n",
        "    tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
        "\n",
        "    out_tokens, _ = generate([tokens], model, max_tokens=300, temperature=0.7, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
        "\n",
        "    result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
        "    regex_parenthess = r\"\\((.*?)\\)\"\n",
        "    regex_codes = r\"^[a-zA-Z]\\d+\"\n",
        "    codes_pred = re.findall(regex_parenthess, result)\n",
        "    codes_pred = [code for code in codes_pred if re.match(regex_codes, code)]\n",
        "    codes_crh = re.findall(regex_parenthess, data[\"messages\"][2][\"content\"])\n",
        "    codes_crh = [code for code in codes_crh if re.match(regex_codes, code)]\n",
        "    vp = [ code for code in codes_pred if code in codes_crh]\n",
        "    fp = [ code for code in codes_pred if code not in codes_crh]\n",
        "    fn = [ code for code in codes_crh if code not in codes_pred]\n",
        "\n",
        "    result_tmp = pd.DataFrame([{\n",
        "       \"text\":data[\"messages\"][1][\"content\"],\n",
        "       \"text_codes_crh\": data[\"messages\"][2][\"content\"],\n",
        "       \"codes_crh\":codes_crh,\n",
        "       \"text_codes_crh\": result,\n",
        "       \"prediction\" :codes_pred,\n",
        "       \"n_codes\" : len(codes_crh),\n",
        "       \"n_pred\" : len(codes_pred),\n",
        "       \"vp\": vp,\n",
        "       \"n_vp\": len(vp),\n",
        "       \"fp\": fp,\n",
        "       \"n_fp\": len(fp),\n",
        "       \"fn\": fn,\n",
        "       \"n_fn\": len(fn)}])\n",
        "    return result_tmp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "limit = 1000000\n",
        "result = pd.DataFrame()\n",
        "print(dt.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")  +\" - Begin prediction loop\")\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "    for idx, line in tqdm(enumerate(lines), total=len(lines)):\n",
        "\n",
        "        data = json.loads(line)\n",
        "        result_tmp = pred_codes_stat(data,tokenizer,model)\n",
        "        result = pd.concat([result, result_tmp])\n",
        "\n",
        "        if i>limit:\n",
        "            break\n",
        "        i+=1\n",
        "\n",
        "print(dt.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")  +\" - End prediction loop\")\n",
        "timeStamp = dt.datetime.today().strftime(\"_%Y-%m-%d-%H%M%S\")\n",
        "file_name = \"test_results\"+timeStamp+\".csv\"\n",
        "result.to_csv(\"/content/recode-with-mistral-finetune/sample_data/\"+file_name)\n",
        "print(\"File \"+file_name  + \" saved\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-RZkw_90RPSx",
        "outputId": "642125a4-b50f-4d82-b30f-691296c26ca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-30 16:28:19 - Begin prediction loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:49<00:00,  7.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-30 16:29:09 - End prediction loop\n",
            "File test_results_2024-12-30-162909.csv saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = np.sum(result[\"n_vp\"]) / np.sum(result[\"n_codes\"])\n",
        "recall = np.sum(result[\"n_vp\"]) / np.sum(result[\"n_pred\"])\n",
        "f1_score = 2 * precision * recall / (precision + recall)\n",
        "acc = result[(result.n_fp ==0) & (result.n_fn==0)].shape[0] / result.shape[0]\n",
        "print(\"Precision = \"  + str(precision))\n",
        "print(\"Recall = \"  + str(recall))\n",
        "print(\"F1-Score = \"  + str(f1_score))\n",
        "print(\"Accuracy = \"  + str(acc))"
      ],
      "metadata": {
        "id": "bQu0p6EpY0Bt",
        "outputId": "82259bc4-f96f-47bc-aa7b-f6e0220332b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision = 0.7333333333333333\n",
            "Recall = 0.7096774193548387\n",
            "F1-Score = 0.7213114754098361\n",
            "Accuracy = 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dt.datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")  +\" - End prediction loop\")\n",
        "timeStamp = dt.datetime.today().strftime(\"_%Y-%m-%d-%H%M%S\")\n",
        "result.to_csv(\"/content/recode-with-mistral-finetune/sample_data/test_results\"+timeStamp+\".csv\")\n",
        "print(\"File test_results\"+timeStamp + \" saved\")"
      ],
      "metadata": {
        "id": "VV8DpTQyXhuf",
        "outputId": "e00258ac-6798-424b-8b83-bb64a56ec598",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-30 16:23:37 - End prediction loop\n",
            "File test_results_2024-12-30-162337 saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "MbpPwp8AYU0F",
        "outputId": "a2798abb-c887-44d6-e877-cbeb5b31fe21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  Générez le codage CIM-10 du résumé strandisé d...   \n",
              "0  Générez le codage CIM-10 du résumé strandisé d...   \n",
              "0  Générez le codage CIM-10 du résumé strandisé d...   \n",
              "0  Générez le codage CIM-10 du résumé strandisé d...   \n",
              "0  Générez le codage CIM-10 du résumé strandisé d...   \n",
              "0  Générez le codage CIM-10 du résumé strandisé d...   \n",
              "0  Générez le codage CIM-10 du résumé strandisé d...   \n",
              "\n",
              "                                      text_codes_crh  \\\n",
              "0  Codes CIM 10 retenus pour le résumé strandisé ...   \n",
              "0  Codes CIM 10 retenus pour le résumé strandisé ...   \n",
              "0  Codes CIM 10 retenus pour le résumé strandisé ...   \n",
              "0  Codes CIM 10 retenus pour le résumé strandisé ...   \n",
              "0  Codes CIM 10 retenus pour le résumé strandisé ...   \n",
              "0  Codes CIM 10 retenus pour le résumé strandisé ...   \n",
              "0  Codes CIM 10 retenus pour le résumé strandisé ...   \n",
              "\n",
              "                                          codes_crh  \\\n",
              "0                   [R102, E1198, E8768, I10, C795]   \n",
              "0                         [C798, E559, N185, E8768]   \n",
              "0                                    [Z511, C189+0]   \n",
              "0  [Z511, C254, I255, E8718, R33, E1190, I10, N185]   \n",
              "0                                 [Z511, C187, R33]   \n",
              "0                          [Z511, C793, C787, G473]   \n",
              "0                           [R101, C787, N185, R33]   \n",
              "\n",
              "                                    prediction  n_codes  n_pred  \\\n",
              "0              [R410, E8768, E1198, I10, C795]        5       5   \n",
              "0                    [C795, E559, N185, E8768]        4       4   \n",
              "0                           [Z511, C795, D124]        2       3   \n",
              "0  [Z511, C797, E1190, N185, E8718, R33, I255]        8       7   \n",
              "0                            [Z511, C187, R33]        3       3   \n",
              "0               [Z511, C798, C795, C787, G473]        4       5   \n",
              "0                      [R101, C787, R33, N185]        4       4   \n",
              "\n",
              "                                      vp  n_vp            fp  n_fp  \\\n",
              "0              [E8768, E1198, I10, C795]     4        [R410]     1   \n",
              "0                    [E559, N185, E8768]     3        [C795]     1   \n",
              "0                                 [Z511]     1  [C795, D124]     2   \n",
              "0  [Z511, E1190, N185, E8718, R33, I255]     6        [C797]     1   \n",
              "0                      [Z511, C187, R33]     3            []     0   \n",
              "0                     [Z511, C787, G473]     3  [C798, C795]     2   \n",
              "0                [R101, C787, R33, N185]     4            []     0   \n",
              "\n",
              "            fn  n_fn  \n",
              "0       [R102]     1  \n",
              "0       [C798]     1  \n",
              "0     [C189+0]     1  \n",
              "0  [C254, I10]     2  \n",
              "0           []     0  \n",
              "0       [C793]     1  \n",
              "0           []     0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e880c03-ea02-4c11-86c1-7343ef9e1cd9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_codes_crh</th>\n",
              "      <th>codes_crh</th>\n",
              "      <th>prediction</th>\n",
              "      <th>n_codes</th>\n",
              "      <th>n_pred</th>\n",
              "      <th>vp</th>\n",
              "      <th>n_vp</th>\n",
              "      <th>fp</th>\n",
              "      <th>n_fp</th>\n",
              "      <th>fn</th>\n",
              "      <th>n_fn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Générez le codage CIM-10 du résumé strandisé d...</td>\n",
              "      <td>Codes CIM 10 retenus pour le résumé strandisé ...</td>\n",
              "      <td>[R102, E1198, E8768, I10, C795]</td>\n",
              "      <td>[R410, E8768, E1198, I10, C795]</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>[E8768, E1198, I10, C795]</td>\n",
              "      <td>4</td>\n",
              "      <td>[R410]</td>\n",
              "      <td>1</td>\n",
              "      <td>[R102]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Générez le codage CIM-10 du résumé strandisé d...</td>\n",
              "      <td>Codes CIM 10 retenus pour le résumé strandisé ...</td>\n",
              "      <td>[C798, E559, N185, E8768]</td>\n",
              "      <td>[C795, E559, N185, E8768]</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>[E559, N185, E8768]</td>\n",
              "      <td>3</td>\n",
              "      <td>[C795]</td>\n",
              "      <td>1</td>\n",
              "      <td>[C798]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Générez le codage CIM-10 du résumé strandisé d...</td>\n",
              "      <td>Codes CIM 10 retenus pour le résumé strandisé ...</td>\n",
              "      <td>[Z511, C189+0]</td>\n",
              "      <td>[Z511, C795, D124]</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>[Z511]</td>\n",
              "      <td>1</td>\n",
              "      <td>[C795, D124]</td>\n",
              "      <td>2</td>\n",
              "      <td>[C189+0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Générez le codage CIM-10 du résumé strandisé d...</td>\n",
              "      <td>Codes CIM 10 retenus pour le résumé strandisé ...</td>\n",
              "      <td>[Z511, C254, I255, E8718, R33, E1190, I10, N185]</td>\n",
              "      <td>[Z511, C797, E1190, N185, E8718, R33, I255]</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>[Z511, E1190, N185, E8718, R33, I255]</td>\n",
              "      <td>6</td>\n",
              "      <td>[C797]</td>\n",
              "      <td>1</td>\n",
              "      <td>[C254, I10]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Générez le codage CIM-10 du résumé strandisé d...</td>\n",
              "      <td>Codes CIM 10 retenus pour le résumé strandisé ...</td>\n",
              "      <td>[Z511, C187, R33]</td>\n",
              "      <td>[Z511, C187, R33]</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>[Z511, C187, R33]</td>\n",
              "      <td>3</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Générez le codage CIM-10 du résumé strandisé d...</td>\n",
              "      <td>Codes CIM 10 retenus pour le résumé strandisé ...</td>\n",
              "      <td>[Z511, C793, C787, G473]</td>\n",
              "      <td>[Z511, C798, C795, C787, G473]</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>[Z511, C787, G473]</td>\n",
              "      <td>3</td>\n",
              "      <td>[C798, C795]</td>\n",
              "      <td>2</td>\n",
              "      <td>[C793]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Générez le codage CIM-10 du résumé strandisé d...</td>\n",
              "      <td>Codes CIM 10 retenus pour le résumé strandisé ...</td>\n",
              "      <td>[R101, C787, N185, R33]</td>\n",
              "      <td>[R101, C787, R33, N185]</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>[R101, C787, R33, N185]</td>\n",
              "      <td>4</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e880c03-ea02-4c11-86c1-7343ef9e1cd9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5e880c03-ea02-4c11-86c1-7343ef9e1cd9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5e880c03-ea02-4c11-86c1-7343ef9e1cd9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-77937781-2810-4767-a0d2-6430aa8527f2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77937781-2810-4767-a0d2-6430aa8527f2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-77937781-2810-4767-a0d2-6430aa8527f2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_77a66011-5917-4646-9d8c-9bcdfd37e72d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('result')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_77a66011-5917-4646-9d8c-9bcdfd37e72d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('result');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result",
              "summary": "{\n  \"name\": \"result\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"G\\u00e9n\\u00e9rez le codage CIM-10 du r\\u00e9sum\\u00e9 strandis\\u00e9 de sortie PMSI \\u00e0 partir du compte rendu d'hospitalisation suivant : **Compte Rendu d'Hospitalisation**\\n\\n**Identit\\u00e9 du patient :**\\n\\nNom : Dupont\\nPr\\u00e9nom : Jean\\n\\u00c2ge : 62 ans\\nSexe : Masculin\\nMode d'entr\\u00e9e : Urgences\\n\\n**Dur\\u00e9e de l'hospitalisation :** 4 jours\\n\\n**Service :** M\\u00e9decine Interne\\n\\n**Motif d'admission :**\\nJean Dupont, un homme de 62 ans, a \\u00e9t\\u00e9 admis aux urgences en raison de douleurs pelviennes et p\\u00e9rin\\u00e9ales intenses qui l'ont r\\u00e9veill\\u00e9 en pleine nuit. Ces douleurs \\u00e9taient accompagn\\u00e9es d'une sensation de malaise g\\u00e9n\\u00e9ral et d'une faiblesse musculaire marqu\\u00e9e.\\n\\n**Ant\\u00e9c\\u00e9dents m\\u00e9dicaux :**\\nJean a un pass\\u00e9 m\\u00e9dical charg\\u00e9. Il souffre de diab\\u00e8te de type 2, trait\\u00e9 par des antidiab\\u00e9tiques oraux, et d'une hypertension essentielle qui n\\u00e9cessite une surveillance r\\u00e9guli\\u00e8re. R\\u00e9cemment, il a \\u00e9t\\u00e9 diagnostiqu\\u00e9 avec une tumeur maligne secondaire des os et de la moelle osseuse, une nouvelle qui a boulevers\\u00e9 sa vie et celle de ses proches. De plus, il pr\\u00e9sente des \\u00e9pisodes r\\u00e9currents d'hypokali\\u00e9mie, ce qui complique encore son \\u00e9tat de sant\\u00e9.\\n\\n**Examen clinique \\u00e0 l'admission :**\\n\\u00c0 son arriv\\u00e9e, Jean pr\\u00e9sentait des signes vitaux instables. Sa pression art\\u00e9rielle \\u00e9tait \\u00e9lev\\u00e9e \\u00e0 160/100 mmHg, et sa fr\\u00e9quence cardiaque \\u00e9tait rapide \\u00e0 105 battements par minute. L'examen physique a r\\u00e9v\\u00e9l\\u00e9 des douleurs pelviennes et p\\u00e9rin\\u00e9ales intenses, ainsi qu'une faiblesse musculaire g\\u00e9n\\u00e9ralis\\u00e9e. Jean semblait \\u00e9puis\\u00e9 et en d\\u00e9tresse.\\n\\n**R\\u00e9sultats des examens compl\\u00e9mentaires :**\\nLes analyses de laboratoire ont montr\\u00e9 des niveaux de potassium s\\u00e9rique tr\\u00e8s bas, confirmant une hypokali\\u00e9mie s\\u00e9v\\u00e8re. Les r\\u00e9sultats \\u00e9taient les suivants :\\n\\n- Potassium : 2.8 mmol/L (valeur normale : 3.5-5.0 mmol/L)\\n- Glyc\\u00e9mie : 200 mg/dL (valeur normale : < 140 mg/dL)\\n- H\\u00e9moglobine A1c : 8.2% (valeur normale : < 5.7%)\\n- Cr\\u00e9atinine : 1.4 mg/dL (valeur normale : < 1.3 mg/dL)\\n- Cholest\\u00e9rol total : 220 mg/dL (valeur normale : < 200 mg/dL)\\n- LDL : 140 mg/dL (valeur normale : < 100 mg/dL)\\n- HDL : 35 mg/dL (valeur normale : > 40 mg/dL)\\n- Triglyc\\u00e9rides : 180 mg/dL (valeur normale : < 150 mg/dL)\\n\\nL'\\u00e9chographie abdominale a r\\u00e9v\\u00e9l\\u00e9 des signes d'inflammation pelvienne, et une IRM a confirm\\u00e9 la pr\\u00e9sence de m\\u00e9tastases osseuses, ajoutant une couche de complexit\\u00e9 \\u00e0 son cas.\\n\\n**Pathologies associ\\u00e9es fictives :**\\nJean pr\\u00e9sente \\u00e9galement une insuffisance r\\u00e9nale chronique l\\u00e9g\\u00e8re et une hypercholest\\u00e9rol\\u00e9mie, compliquant encore son \\u00e9tat de sant\\u00e9. Ces conditions n\\u00e9cessitent une surveillance \\u00e9troite et un ajustement constant de son traitement m\\u00e9dical.\\n\\n**Traitement et \\u00e9volution :**\\nJean a re\\u00e7u un traitement m\\u00e9dical intensif d\\u00e8s son admission. Pour corriger l'hypokali\\u00e9mie, il a \\u00e9t\\u00e9 mis sous perfusion de chlorure de potassium \\u00e0 une dose de 10 mmol/h jusqu'\\u00e0 normalisation des taux s\\u00e9riques. Pour g\\u00e9rer son diab\\u00e8te, il a re\\u00e7u de la metformine \\u00e0 une dose de 1000 mg deux fois par jour, et son hypertension a \\u00e9t\\u00e9 contr\\u00f4l\\u00e9e avec de l'amlodipine \\u00e0 10 mg par jour et du lisinopril \\u00e0 20 mg par jour.\\n\\nUne consultation avec l'oncologie a \\u00e9t\\u00e9 organis\\u00e9e pour \\u00e9valuer l'\\u00e9volution de sa tumeur maligne secondaire. Apr\\u00e8s une discussion approfondie, il a \\u00e9t\\u00e9 d\\u00e9cid\\u00e9 de commencer une chimioth\\u00e9rapie cibl\\u00e9e pour ralentir la progression des m\\u00e9tastases osseuses.\\n\\nAu fil des jours, l'\\u00e9tat g\\u00e9n\\u00e9ral de Jean s'est progressivement am\\u00e9lior\\u00e9. Ses douleurs pelviennes et p\\u00e9rin\\u00e9ales ont diminu\\u00e9 gr\\u00e2ce \\u00e0 des analg\\u00e9siques puissants, et ses niveaux de potassium se sont stabilis\\u00e9s. La vigilance et les soins attentifs de l'\\u00e9quipe m\\u00e9dicale ont \\u00e9t\\u00e9 cruciaux pour son r\\u00e9tablissement.\\n\\n**Conclusion et recommandations :**\\nApr\\u00e8s une hospitalisation de 4 jours, Jean a pu quitter l'h\\u00f4pital avec un diagnostic final de douleurs pelviennes et p\\u00e9rin\\u00e9ales associ\\u00e9es \\u00e0 une hypokali\\u00e9mie s\\u00e9v\\u00e8re, un diab\\u00e8te de type 2, une hypertension essentielle et une tumeur maligne secondaire des os et de la moelle osseuse.\\n\\nPour assurer un suivi optimal, il est recommand\\u00e9 de surveiller r\\u00e9guli\\u00e8rement ses niveaux de potassium, de suivre de pr\\u00e8s l'\\u00e9volution de son diab\\u00e8te et de son hypertension, et de continuer le traitement m\\u00e9dical prescrit. Des visites de suivi r\\u00e9guli\\u00e8res avec son oncologue sont \\u00e9galement n\\u00e9cessaires pour \\u00e9valuer l'efficacit\\u00e9 de la chimioth\\u00e9rapie et ajuster le traitement en cons\\u00e9quence.\\n\\nJean a \\u00e9t\\u00e9 encourag\\u00e9 \\u00e0 adopter un mode de vie plus sain, incluant une alimentation \\u00e9quilibr\\u00e9e et une activit\\u00e9 physique mod\\u00e9r\\u00e9e, pour am\\u00e9liorer sa qualit\\u00e9 de vie et r\\u00e9duire les risques de complications futures.\",\n          \"G\\u00e9n\\u00e9rez le codage CIM-10 du r\\u00e9sum\\u00e9 strandis\\u00e9 de sortie PMSI \\u00e0 partir du compte rendu d'hospitalisation suivant : **Compte Rendu d'Hospitalisation**\\n\\n**Identit\\u00e9 du patient :**\\n\\nNom : Dupont\\nPr\\u00e9nom : Jean\\n\\u00c2ge : 65 ans\\nSexe : Masculin\\nMode d'entr\\u00e9e : Urgences\\n\\n**Dur\\u00e9e de l'hospitalisation :** 3 jours\\n\\n**Service :** Oncologie\\n\\n**Motif d'admission :**\\nJean Dupont, un homme de 65 ans, a \\u00e9t\\u00e9 admis aux urgences en raison de douleurs intenses et de faiblesses g\\u00e9n\\u00e9ralis\\u00e9es. Il avait r\\u00e9cemment \\u00e9t\\u00e9 diagnostiqu\\u00e9 avec une tumeur maligne secondaire, ce qui a n\\u00e9cessit\\u00e9 une attention imm\\u00e9diate.\\n\\n**Ant\\u00e9c\\u00e9dents m\\u00e9dicaux :**\\nJean a un pass\\u00e9 m\\u00e9dical complexe, marqu\\u00e9 par une carence en vitamine D, une maladie r\\u00e9nale chronique au stade 5 et des hypokali\\u00e9mies. Ces conditions ont compliqu\\u00e9 son \\u00e9tat de sant\\u00e9 et n\\u00e9cessit\\u00e9 une surveillance \\u00e9troite.\\n\\n**Examen clinique \\u00e0 l'admission :**\\n\\u00c0 son arriv\\u00e9e, Jean pr\\u00e9sentait des signes vitaux instables, avec une pression art\\u00e9rielle \\u00e9lev\\u00e9e et une fr\\u00e9quence cardiaque rapide. L'examen physique a r\\u00e9v\\u00e9l\\u00e9 des douleurs thoraciques intenses et des signes de d\\u00e9tresse respiratoire.\\n\\n**R\\u00e9sultats des examens compl\\u00e9mentaires :**\\nLes analyses de laboratoire ont montr\\u00e9 des niveaux \\u00e9lev\\u00e9s de troponine, confirmant une tumeur maligne secondaire. L'\\u00e9lectrocardiogramme a r\\u00e9v\\u00e9l\\u00e9 des anomalies significatives, et l'\\u00e9chocardiographie a montr\\u00e9 une r\\u00e9duction de la fonction ventriculaire gauche.\\n\\n**R\\u00e9sultats biologiques :**\\n- Vitamine D : 15 ng/mL (valeur normale > 30 ng/mL)\\n- Cr\\u00e9atinine : 4.5 mg/dL (valeur normale < 1.3 mg/dL)\\n- Potassium : 3.2 mmol/L (valeur normale 3.5-5.0 mmol/L)\\n- H\\u00e9moglobine A1c : 7.5% (valeur normale < 5.7%)\\n- Calcium : 8.5 mg/dL (valeur normale 8.6-10.2 mg/dL)\\n- Phosphore : 5.5 mg/dL (valeur normale 2.7-4.5 mg/dL)\\n\\n**Traitement et \\u00e9volution :**\\nJean a re\\u00e7u un traitement m\\u00e9dical intensif, incluant des anticoagulants, des b\\u00eata-bloquants et des inhibiteurs de l'enzyme de conversion. Une angioplastie coronarienne a \\u00e9t\\u00e9 r\\u00e9alis\\u00e9e pour r\\u00e9tablir le flux sanguin dans les art\\u00e8res coronaires. Au fil des jours, son \\u00e9tat g\\u00e9n\\u00e9ral s'est progressivement am\\u00e9lior\\u00e9, gr\\u00e2ce \\u00e0 la vigilance et aux soins attentifs de l'\\u00e9quipe m\\u00e9dicale.\\n\\n**Conclusion et recommandations :**\\nApr\\u00e8s une hospitalisation de 3 jours, Jean a pu quitter l'h\\u00f4pital avec un diagnostic final de tumeur maligne secondaire. Pour assurer un suivi optimal, il est recommand\\u00e9 de surveiller r\\u00e9guli\\u00e8rement la fonction cardiaque, de suivre de pr\\u00e8s l'\\u00e9volution de la maladie r\\u00e9nale chronique et de continuer le traitement m\\u00e9dical prescrit. Des visites de suivi r\\u00e9guli\\u00e8res sont \\u00e9galement n\\u00e9cessaires pour s'assurer qu'il n'y a pas de r\\u00e9cidive de la tumeur maligne secondaire.\",\n          \"G\\u00e9n\\u00e9rez le codage CIM-10 du r\\u00e9sum\\u00e9 strandis\\u00e9 de sortie PMSI \\u00e0 partir du compte rendu d'hospitalisation suivant : ### Compte Rendu d'Hospitalisation\\n\\n**Identit\\u00e9 du patient :**\\n\\n**Nom :** Dupont\\n**Pr\\u00e9nom :** Jean\\n**\\u00c2ge :** 62 ans\\n**Sexe :** Masculin\\n**Mode d'entr\\u00e9e :** Consultation externe\\n\\n**Dur\\u00e9e de l'hospitalisation :** 0 jours\\n\\n**Service :** Oncologie\\n\\n---\\n\\n**Motif d'admission :**\\n\\nJean Dupont, un homme de 62 ans, s'est pr\\u00e9sent\\u00e9 \\u00e0 l'h\\u00f4pital pour une s\\u00e9ance de chimioth\\u00e9rapie dans le cadre de son traitement pour une tumeur maligne secondaire du cerveau et des m\\u00e9ninges c\\u00e9r\\u00e9brales. Cette s\\u00e9ance faisait partie d'un protocole de soins intensifs, visant \\u00e0 contenir la progression de la maladie et \\u00e0 am\\u00e9liorer sa qualit\\u00e9 de vie.\\n\\n---\\n\\n**Ant\\u00e9c\\u00e9dents m\\u00e9dicaux :**\\n\\nJean a un pass\\u00e9 m\\u00e9dical charg\\u00e9. Il a \\u00e9t\\u00e9 diagnostiqu\\u00e9 avec une tumeur maligne secondaire du cerveau et des m\\u00e9ninges c\\u00e9r\\u00e9brales, ainsi qu'une tumeur maligne secondaire du foie et des voies biliaires intrah\\u00e9patiques. En plus de ces pathologies graves, il souffre \\u00e9galement d'apn\\u00e9e du sommeil, une condition qui complique encore plus son \\u00e9tat de sant\\u00e9 g\\u00e9n\\u00e9ral.\\n\\n---\\n\\n**Examen clinique \\u00e0 l'admission :**\\n\\n\\u00c0 son arriv\\u00e9e, Jean semblait fatigu\\u00e9 mais d\\u00e9termin\\u00e9 \\u00e0 poursuivre son traitement. Son examen clinique a r\\u00e9v\\u00e9l\\u00e9 une l\\u00e9g\\u00e8re perte de poids et des signes de fatigue chronique. Sa pression art\\u00e9rielle \\u00e9tait stable, mais il pr\\u00e9sentait des signes de d\\u00e9tresse respiratoire l\\u00e9g\\u00e8re, probablement dus \\u00e0 son apn\\u00e9e du sommeil.\\n\\n---\\n\\n**R\\u00e9sultats des examens compl\\u00e9mentaires :**\\n\\nLes analyses de laboratoire ont montr\\u00e9 des niveaux \\u00e9lev\\u00e9s de marqueurs tumoraux, confirmant la pr\\u00e9sence active de la maladie. Une imagerie par r\\u00e9sonance magn\\u00e9tique (IRM) a r\\u00e9v\\u00e9l\\u00e9 des l\\u00e9sions dans le cerveau et le foie, indiquant la progression des tumeurs secondaires. Une \\u00e9valuation de la fonction respiratoire a \\u00e9galement \\u00e9t\\u00e9 r\\u00e9alis\\u00e9e, montrant des interruptions fr\\u00e9quentes de la respiration pendant le sommeil, typiques de l'apn\\u00e9e du sommeil.\\n\\n---\\n\\n**R\\u00e9sultats biologiques :**\\n\\n- **Marqueurs tumoraux :** CEA (Antig\\u00e8ne carcino-embryonnaire) : 20 ng/mL (valeur normale < 5 ng/mL)\\n- **Fonction h\\u00e9patique :**\\n  - ALT (Alanine aminotransf\\u00e9rase) : 60 U/L (valeur normale < 40 U/L)\\n  - AST (Aspartate aminotransf\\u00e9rase) : 50 U/L (valeur normale < 40 U/L)\\n  - Bilirubine totale : 1.5 mg/dL (valeur normale < 1.2 mg/dL)\\n- **Fonction r\\u00e9nale :**\\n  - Cr\\u00e9atinine : 0.9 mg/dL (valeur normale < 1.3 mg/dL)\\n- **H\\u00e9mogramme :**\\n  - H\\u00e9moglobine : 12 g/dL (valeur normale 13.5-17.5 g/dL)\\n  - Plaquettes : 150,000 /\\u00b5L (valeur normale 150,000-450,000 /\\u00b5L)\\n  - Leucocytes : 6,000 /\\u00b5L (valeur normale 4,000-11,000 /\\u00b5L)\\n\\n---\\n\\n**Traitement et \\u00e9volution :**\\n\\nJean a re\\u00e7u une s\\u00e9ance de chimioth\\u00e9rapie avec une combinaison de cisplatine (75 mg/m\\u00b2) et de gemcitabine (1,000 mg/m\\u00b2). La s\\u00e9ance s'est d\\u00e9roul\\u00e9e sans complications majeures, bien que Jean ait ressenti une l\\u00e9g\\u00e8re naus\\u00e9e et une fatigue accrue apr\\u00e8s le traitement. Des m\\u00e9dicaments antinaus\\u00e9eux, tels que l'ondans\\u00e9tron (8 mg), ont \\u00e9t\\u00e9 administr\\u00e9s pour soulager ces sympt\\u00f4mes.\\n\\nEn plus de la chimioth\\u00e9rapie, Jean a \\u00e9t\\u00e9 conseill\\u00e9 sur l'importance de continuer \\u00e0 utiliser son appareil de pression positive continue (CPAP) pour g\\u00e9rer son apn\\u00e9e du sommeil, afin de minimiser les effets n\\u00e9fastes sur sa sant\\u00e9 respiratoire et cardiaque.\\n\\n---\\n\\n**Conclusion et recommandations :**\\n\\nApr\\u00e8s la s\\u00e9ance de chimioth\\u00e9rapie, Jean a pu rentrer chez lui le jour m\\u00eame. Il est crucial de continuer \\u00e0 surveiller l'\\u00e9volution de ses tumeurs secondaires et de suivre de pr\\u00e8s les effets secondaires de la chimioth\\u00e9rapie. Des visites de suivi r\\u00e9guli\\u00e8res sont n\\u00e9cessaires pour ajuster le traitement et s'assurer qu'il n'y a pas de complications.\\n\\nIl est \\u00e9galement recommand\\u00e9 de maintenir une communication \\u00e9troite avec l'\\u00e9quipe m\\u00e9dicale pour toute question ou pr\\u00e9occupation concernant son \\u00e9tat de sant\\u00e9. Jean doit continuer \\u00e0 utiliser son appareil CPAP et suivre un r\\u00e9gime alimentaire \\u00e9quilibr\\u00e9 pour soutenir son syst\\u00e8me immunitaire et sa sant\\u00e9 globale.\\n\\n---\\n\\n**Sign\\u00e9 :**\\n\\nDr. Marie Dubois\\nOncologue\\nH\\u00f4pital Universitaire de Paris\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_codes_crh\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Codes CIM 10 retenus pour le r\\u00e9sum\\u00e9 strandis\\u00e9 de sortie PMSI : Diagnostic principal : Douleur pelvienne et p\\u00e9rin\\u00e9ale (R410)\\nDiagnostic reli\\u00e9 : Aucun.\\nDiagnostics associ\\u00e9s :\\nHypokali\\u00e9mies, autres et sans pr\\u00e9cision (E8768),\\nDiab\\u00e8te sucr\\u00e9 de type 2 non insulinotrait\\u00e9 ou sans pr\\u00e9cision, sans complication (E1198),\\nHypertension essentielle (primitive) (I10),\\nTumeur maligne secondaire des os et de la moelle osseuse (C795).\",\n          \"Codes CIM 10 retenus pour le r\\u00e9sum\\u00e9 strandis\\u00e9 de sortie PMSI : Diagnostic principal : Tumeur maligne secondaire du myocardium (C795)\\nDiagnostic reli\\u00e9 : Aucun.\\nDiagnostics associ\\u00e9s :\\nCarence en vitamine D, sans pr\\u00e9cision (E559),\\nMaladie r\\u00e9nale chronique, stade 5 (N185),\\nHypokali\\u00e9mies, autres et sans pr\\u00e9cision (E8768).\",\n          \"Codes CIM 10 retenus pour le r\\u00e9sum\\u00e9 strandis\\u00e9 de sortie PMSI : Diagnostic principal : S\\u00e9ance de chimioth\\u00e9rapie pour tumeur (Z511).\\nDiagnostic reli\\u00e9 : Tumeur maligne secondaire d'autres si\\u00e8ges pr\\u00e9cis\\u00e9s (C798).\\nDiagnostics associ\\u00e9s :\\nTumeur maligne secondaire du cerveau et des m\\u00e9ninges c\\u00e9r\\u00e9brales (C795),\\nTumeur maligne secondaire du foie et des voies biliaires intrah\\u00e9patiques (C787),\\nApn\\u00e9e du sommeil (G473).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"codes_crh\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prediction\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_codes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 8,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 7,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4,\n          7,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_vp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          6,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_fp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fn\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_fn\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W_hWL9LhYZsu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9bb325daa4e468ca6692a5fa56fa746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_7a8a1720cd724e87ae7adc6e022367b7"
          }
        },
        "756aefba131c4f96b38b91189e75388c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_330828c363554b3793c8dd6b552ec1e4",
            "placeholder": "​",
            "style": "IPY_MODEL_9a95b97f66ea452c9618a79538cb5c27",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "2bbbcfba8d4b44faa10513dc0c4fedb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_a5f08c3c23904be7965d3393ab69b5eb",
            "placeholder": "​",
            "style": "IPY_MODEL_d76c4a231cee4a5f8a64fdea87fcbb29",
            "value": ""
          }
        },
        "c9c91aaf3c8a49cea2cfed5ebd380dcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_5867f4d42ea547fab863ea19186d3e2a",
            "style": "IPY_MODEL_ed61d658b5d549e4bb42abc1e12e6c39",
            "value": true
          }
        },
        "796dcd2a671349c2bc52a138f34a7f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4d38049c50ad485c8d554c20bfbf5db4",
            "style": "IPY_MODEL_583231823e90487690e4a32d119db6f9",
            "tooltip": ""
          }
        },
        "b53c43d389c24541b9529bfe916471d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6928ca3cbd3b4ff49ed42fa648f8e57c",
            "placeholder": "​",
            "style": "IPY_MODEL_0b068494b0094ee3a3c9ea601ad7b145",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "7a8a1720cd724e87ae7adc6e022367b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "330828c363554b3793c8dd6b552ec1e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a95b97f66ea452c9618a79538cb5c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5f08c3c23904be7965d3393ab69b5eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d76c4a231cee4a5f8a64fdea87fcbb29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5867f4d42ea547fab863ea19186d3e2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed61d658b5d549e4bb42abc1e12e6c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d38049c50ad485c8d554c20bfbf5db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "583231823e90487690e4a32d119db6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "6928ca3cbd3b4ff49ed42fa648f8e57c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b068494b0094ee3a3c9ea601ad7b145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38a5c2c081f14330a7eab7535113523f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_506cd5ade01945b4af5f6f21d27fe3c6",
            "placeholder": "​",
            "style": "IPY_MODEL_4929556aee13456bb02efeec925f24d3",
            "value": "Connecting..."
          }
        },
        "506cd5ade01945b4af5f6f21d27fe3c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4929556aee13456bb02efeec925f24d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a550218d4314318a7d96442049568c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90a3e15f8bd742a681649132e7792304",
              "IPY_MODEL_474d9603ae624e40a6ff3bcf6d1c98de",
              "IPY_MODEL_3c2d923788754ed4bd27f45cf8d1d62b"
            ],
            "layout": "IPY_MODEL_8d122899fb6a4b8a8cca4f1276b89a6f"
          }
        },
        "90a3e15f8bd742a681649132e7792304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4faa0da877664304a52954e037f9f3a6",
            "placeholder": "​",
            "style": "IPY_MODEL_9ff61767564148fdbc14ebf2b60b4a96",
            "value": "Fetching 3 files: 100%"
          }
        },
        "474d9603ae624e40a6ff3bcf6d1c98de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d70907b99fec4ba98cdc5af471bf7156",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6724778febe74ce6b582e40b077e2806",
            "value": 3
          }
        },
        "3c2d923788754ed4bd27f45cf8d1d62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f637a4487d6040d2b87f6e796a38b5ce",
            "placeholder": "​",
            "style": "IPY_MODEL_2df3eb022736446885dc084e00ee10ee",
            "value": " 3/3 [05:45&lt;00:00, 345.11s/it]"
          }
        },
        "8d122899fb6a4b8a8cca4f1276b89a6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4faa0da877664304a52954e037f9f3a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff61767564148fdbc14ebf2b60b4a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d70907b99fec4ba98cdc5af471bf7156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6724778febe74ce6b582e40b077e2806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f637a4487d6040d2b87f6e796a38b5ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df3eb022736446885dc084e00ee10ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56f6b8c54a334d7bbd9790382e4d6280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5924b6828df41dbb86de20d28fe21e5",
              "IPY_MODEL_68085d9395bf4bc68b7e092508079246",
              "IPY_MODEL_9871ea5e8ddf428485a396e9b710fcb5"
            ],
            "layout": "IPY_MODEL_7cf9ab73cefd4e72ae95c1150a045a56"
          }
        },
        "a5924b6828df41dbb86de20d28fe21e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13c383dfda34aa3822659180463ce72",
            "placeholder": "​",
            "style": "IPY_MODEL_7dc9ca18eb0f483281a8257252378600",
            "value": "tokenizer.model.v3: 100%"
          }
        },
        "68085d9395bf4bc68b7e092508079246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80e0eae7b7e445d8856ef4510f8cb39c",
            "max": 587404,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3b85a006fa14f6f864587878fa29950",
            "value": 587404
          }
        },
        "9871ea5e8ddf428485a396e9b710fcb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30ba41a57589421ea787fc9b41a63b24",
            "placeholder": "​",
            "style": "IPY_MODEL_cc5150ea22094da081a256cb693cff73",
            "value": " 587k/587k [00:00&lt;00:00, 9.67MB/s]"
          }
        },
        "7cf9ab73cefd4e72ae95c1150a045a56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a13c383dfda34aa3822659180463ce72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc9ca18eb0f483281a8257252378600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80e0eae7b7e445d8856ef4510f8cb39c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b85a006fa14f6f864587878fa29950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30ba41a57589421ea787fc9b41a63b24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc5150ea22094da081a256cb693cff73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a14f6a855fd4323a68d2138d2f37c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a9386c85aed474e96aad3ee691c394d",
              "IPY_MODEL_d15d59c9d13f4134abdca1a4e5449dce",
              "IPY_MODEL_59781676db3f4484a7424b775245fe83"
            ],
            "layout": "IPY_MODEL_097e8edec8a44cf6b97b771c5af95162"
          }
        },
        "3a9386c85aed474e96aad3ee691c394d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6183e799cb94441a11c2d2049dc29c6",
            "placeholder": "​",
            "style": "IPY_MODEL_a6c600f3cab3474baf97000ec604f37a",
            "value": "params.json: 100%"
          }
        },
        "d15d59c9d13f4134abdca1a4e5449dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3d753c1ffc44f0da7b091fd985c52f1",
            "max": 202,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ca757a6a84b45dbbd41fc3bb3cfbdde",
            "value": 202
          }
        },
        "59781676db3f4484a7424b775245fe83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2de530c6121a49ca97ab4f7a1879da43",
            "placeholder": "​",
            "style": "IPY_MODEL_265f2850068b472a894463f0a794a44f",
            "value": " 202/202 [00:00&lt;00:00, 15.8kB/s]"
          }
        },
        "097e8edec8a44cf6b97b771c5af95162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6183e799cb94441a11c2d2049dc29c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6c600f3cab3474baf97000ec604f37a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3d753c1ffc44f0da7b091fd985c52f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ca757a6a84b45dbbd41fc3bb3cfbdde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2de530c6121a49ca97ab4f7a1879da43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "265f2850068b472a894463f0a794a44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0661b35f098a4d0380e16bbb40dc3693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_091962f16ed64c5bae40ff4809e62a71",
              "IPY_MODEL_e3a5ad5b0d9449398bcec6c29cc6cddf",
              "IPY_MODEL_4d812b5707e7412b9f36fbc9ccef36a0"
            ],
            "layout": "IPY_MODEL_548f7ae76dd44dfa80bab8a5b5fc0df9"
          }
        },
        "091962f16ed64c5bae40ff4809e62a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_389a7a2aee864da5afaec1e5f78a8db7",
            "placeholder": "​",
            "style": "IPY_MODEL_a8587dd9827747a9883ed35f6499b881",
            "value": "consolidated.safetensors: 100%"
          }
        },
        "e3a5ad5b0d9449398bcec6c29cc6cddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a538515eff84884b846f223df4f5eb9",
            "max": 14496078512,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30328e21bf894c9fae5d04a7d646e579",
            "value": 14496078512
          }
        },
        "4d812b5707e7412b9f36fbc9ccef36a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e78a16e514a24719aa4f32438b3d023b",
            "placeholder": "​",
            "style": "IPY_MODEL_4eed08fff768493892287a00abb451ad",
            "value": " 14.5G/14.5G [05:44&lt;00:00, 42.3MB/s]"
          }
        },
        "548f7ae76dd44dfa80bab8a5b5fc0df9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "389a7a2aee864da5afaec1e5f78a8db7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8587dd9827747a9883ed35f6499b881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a538515eff84884b846f223df4f5eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30328e21bf894c9fae5d04a7d646e579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e78a16e514a24719aa4f32438b3d023b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eed08fff768493892287a00abb451ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}